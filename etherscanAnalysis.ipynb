{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethereum Transaction Data Analysis with Python and PySpark\n",
    "\n",
    "This notebook, \"etherscanAnalysis.ipynb\", is designed for analyzing Ethereum transaction data. It employs a combination of Python standard libraries and PySpark, a powerful tool for large-scale data processing. The primary steps in the notebook are:\n",
    "\n",
    "1. **Initial Setup**: The notebook begins by importing necessary libraries and setting paths for input and output files containing Ethereum transaction data.\n",
    "2. **Data Processing with Python**: It includes Python scripts for handling large CSV files, defining and retaining key transaction attributes such as addresses, timestamps, and values. The data is filtered based on these attributes and saved to a new CSV file. Additionally, it counts and prints the number of rows based on a specific condition in the dataset.\n",
    "3. **Data Processing Using PySpark**: Leveraging PySpark's distributed computing capabilities, the notebook reads the CSV data into a Spark DataFrame, performs data transformations, and writes the processed data to a new file. The notebook also uses PySpark to count and filter rows based on certain conditions.\n",
    "\n",
    "This notebook is an effective tool for extracting, filtering, and processing large volumes of Ethereum blockchain transaction data, showcasing the synergy between Python and PySpark for data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "input_file = '/mnt/c/Users/korchien/Downloads/server-3-152.7.177.227/ExternalTransactionItem.csv'\n",
    "output_file = '/mnt/c/Users/korchien/Downloads/server-3-152.7.177.227/FilteredExternalTransactionItem.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the maximum field size limit\n",
    "max_int = sys.maxsize\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(max_int)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        max_int = int(max_int / 2)\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['address_from', 'address_to', 'input', 'timestamp', 'transaction_hash', 'value']\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8') as infile, \\\n",
    "     open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    writer = csv.DictWriter(outfile, fieldnames=columns_to_keep)\n",
    "\n",
    "    # Write the header based on the columns to keep\n",
    "    writer.writeheader()\n",
    "\n",
    "    count_0x = 0  # Initialize a counter for rows with '0x'\n",
    "\n",
    "    for row in reader:\n",
    "        # Check if 'input' column is not '0x'\n",
    "        if row.get('input', '') != '0x':  \n",
    "            # Write only the columns we want to keep\n",
    "            writer.writerow({col: row[col] for col in columns_to_keep})\n",
    "        else:\n",
    "            count_0x += 1\n",
    "\n",
    "print(f\"Number of rows with '0x' in the specified column: {count_0x}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_0x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Filter and Select Columns\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(input_file, header=True, inferSchema=True)\n",
    "\n",
    "# Select only the columns to keep\n",
    "df = df.select(*columns_to_keep)\n",
    "\n",
    "# Count the number of rows where 'input' column is '0x'\n",
    "count_0x = df.filter(col('input') == '0x').count()\n",
    "\n",
    "# Filter out rows where 'input' column is '0x'\n",
    "df_filtered = df.filter(col('input') != '0x')\n",
    "\n",
    "# Write the filtered DataFrame to a new CSV file\n",
    "df_filtered.write.csv(output_file, header=True, mode='overwrite')\n",
    "\n",
    "# Print the count of '0x' rows\n",
    "print(f\"Number of rows with '0x' in the specified column: {count_0x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(output_file, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in the DataFrame\n",
    "row_count = df.count()\n",
    "\n",
    "print(f\"The DataFrame contains {row_count} rows.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
